-- P17: Product Development & Validation - Enhanced Migration
-- 50 days, 10 modules, comprehensive product management framework
-- India-specific: user behavior patterns, vernacular design, low-bandwidth optimization, Indian market pricing

BEGIN;

DO $$
DECLARE
    v_product_id TEXT;
    v_mod_1_id TEXT;
    v_mod_2_id TEXT;
    v_mod_3_id TEXT;
    v_mod_4_id TEXT;
    v_mod_5_id TEXT;
    v_mod_6_id TEXT;
    v_mod_7_id TEXT;
    v_mod_8_id TEXT;
    v_mod_9_id TEXT;
    v_mod_10_id TEXT;
BEGIN
    -- Generate IDs
    v_product_id := gen_random_uuid()::text;
    v_mod_1_id := gen_random_uuid()::text;
    v_mod_2_id := gen_random_uuid()::text;
    v_mod_3_id := gen_random_uuid()::text;
    v_mod_4_id := gen_random_uuid()::text;
    v_mod_5_id := gen_random_uuid()::text;
    v_mod_6_id := gen_random_uuid()::text;
    v_mod_7_id := gen_random_uuid()::text;
    v_mod_8_id := gen_random_uuid()::text;
    v_mod_9_id := gen_random_uuid()::text;
    v_mod_10_id := gen_random_uuid()::text;

    -- Insert/Update Product
    INSERT INTO "Product" (id, code, title, description, price, "estimatedDays", "createdAt", "updatedAt")
    VALUES (
        v_product_id,
        'P17',
        'Product Development & Validation',
        'Master the complete product lifecycle from customer discovery to product-market fit. 50 days, 10 modules covering customer research, MVP design, prototyping, user testing, agile development, feature prioritization, growth experiments, and product analytics with India-specific strategies for vernacular design, low-bandwidth optimization, and Indian market pricing.',
        6999,
        50,
        NOW(),
        NOW()
    )
    ON CONFLICT (code) DO UPDATE SET
        title = EXCLUDED.title,
        description = EXCLUDED.description,
        price = EXCLUDED.price,
        "estimatedDays" = EXCLUDED."estimatedDays",
        "updatedAt" = NOW();

    -- Get the product ID (in case of conflict)
    SELECT id INTO v_product_id FROM "Product" WHERE code = 'P17';

    -- Clean existing modules and lessons for this product
    DELETE FROM "Lesson" WHERE "moduleId" IN (SELECT id FROM "Module" WHERE "productId" = v_product_id);
    DELETE FROM "Module" WHERE "productId" = v_product_id;

    -- Module 1: Customer Discovery (Days 1-5)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_1_id, v_product_id, 'Customer Discovery', 'Master problem interviews, Jobs-to-be-Done framework, and customer segmentation for the Indian market with regional and language-specific insights', 1, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_1_id, 1, 'The Art of Problem Interviews in India',
    'Understanding Indian customers requires navigating unique cultural dynamics where direct negative feedback is rare. Indian consumers across 28 states and 22 official languages have vastly different needs, behaviors, and purchase patterns. The Indian startup ecosystem sees 90% failure rate, with 42% failing due to building products nobody wants. Master the Mom Test adapted for Indian context where family influence on decisions is 2-3x higher than Western markets. Indian customers tend to be agreeable in interviews - learn to read between the lines and use indirect questioning techniques. Regional differences matter significantly: North Indian consumers are more price-sensitive but brand-aspirational, South Indian consumers prioritize quality and education, Western India (Gujarat, Maharashtra) focuses on value and practicality, Eastern India values trust and relationships above all. Interview techniques for India: WhatsApp voice notes work better than formal video calls for Tier 2/3 audiences, Hindi-English code-mixing is natural and builds rapport, family decision-makers often need to be part of the conversation. Set up interviews through warm introductions - cold outreach response rates in India are 3-5% vs 15-20% through referrals. Use local language greetings even if conducting interview in English. Understand that Indian customers may say yes to avoid conflict - probe deeper with specific scenarios and past behavior questions. Price discovery in India requires indirect questioning since discussing money directly is culturally sensitive.',
    '["Create interview script with indirect questioning techniques for Indian cultural context", "Identify 20 potential customers across Tier 1, 2, and 3 cities for diverse insights", "Set up WhatsApp Business for conducting voice note interviews with Tier 2/3 customers", "Develop Hindi and regional language greetings and key question translations"]'::jsonb,
    '{"templates": ["India Customer Interview Script Template", "Regional Customer Mapping Worksheet", "Mom Test Questions Adapted for India", "Interview Consent Form (Hindi/English)"], "tools": ["WhatsApp Business Interview Setup Guide", "Regional Language Phrase Bank", "Customer Discovery Tracker India", "Referral Request Templates"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_1_id, 2, 'Jobs-to-be-Done Framework for Indian Consumers',
    'The Jobs-to-be-Done (JTBD) framework reveals why Indian customers hire products to make progress in their lives. Indian consumer jobs have unique characteristics: status signaling is a powerful emotional job (75% of premium purchases in India are status-driven), family approval is a social job that Western frameworks miss, and value consciousness is embedded in every functional job. Understand the Indian hierarchy of jobs: basic functional needs (does it work?), value assessment (is it worth the price?), social validation (what will others think?), and aspiration fulfillment (does it elevate my status?). The forces of progress in India are unique: push factors include keeping up with peers, family pressure for modern adoption, and FOMO from social media exposure; pull factors include aspiration for Western/premium experiences, convenience in time-poor urban life, and access to previously unavailable products. Anxiety factors are stronger in India: will it work as promised (trust deficit), can I return it (risk aversion), what if I get cheated (fraud concern), and will service be available locally. Habit factors are stronger too: loyalty to local shops, comfort with existing solutions, preference for cash/COD, and resistance to learning new systems. Map job stories in Indian context: "When I want to look successful at my school reunion, I want to buy a recognizable brand smartphone, so I can feel confident and gain respect from former classmates." Indian consumers often hire products for multiple jobs simultaneously - a car is transportation, status symbol, family approval, and investment all at once.',
    '["Identify 5 functional, emotional, and social jobs your Indian customers are trying to accomplish", "Map the 4 forces of progress with India-specific push, pull, anxiety, and habit factors", "Write 10 job stories using Indian consumer scenarios and cultural context", "Document the status and family approval dimensions of your product category"]'::jsonb,
    '{"templates": ["JTBD Canvas for Indian Market", "Forces of Progress India Worksheet", "Job Story Template with Status/Family Dimensions", "Indian Consumer Motivation Map"], "tools": ["Job Priority Calculator India", "Switching Trigger Analyzer", "Anxiety Factor Assessment", "Cultural Job Mapper"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_1_id, 3, 'Customer Segmentation for India''s Diverse Market',
    'India is not one market but many markets layered by geography, language, income, urban-rural divide, and digital maturity. Geographic segmentation: Metro cities (8 cities, 70M population, high digital adoption), Tier 1 (50 cities, 150M population, growing digital), Tier 2 (100 cities, 200M, emerging digital), Tier 3+ and rural (900M population, mobile-first internet). Income segmentation: Elite (>25 LPA, 15M households, premium buyers), Affluent (12-25 LPA, 30M households, quality-conscious), Aspirers (5-12 LPA, 100M households, value-seekers), Next Billion (2-5 LPA, 150M households, need-based buyers). Digital behavior segments: Digital natives (18-30, smartphone-first), Digital migrants (30-45, adapting to digital), Digital immigrants (45+, assisted digital), and Offline-primary (limited digital interaction). Language-based segmentation: English-primary (50M users, metros), Hindi-primary (400M users, North/Central India), Regional language users (350M users, strong in South and East). Create behavioral segments beyond demographics: early adopters (5% willing to try new, pay premium), pragmatic majority (60% need social proof), conservative buyers (30% need extensive convincing), and laggards (5% resist change). Indian segment sizing approach: use NSSO data for income distribution, TRAI data for digital penetration, Census data for geographic spread, and Google/Facebook audience tools for interest-based sizing.',
    '["Create customer segment matrix with geographic, income, digital, and language layers", "Size your addressable segments using NSSO, TRAI, and Census data sources", "Develop 5 detailed customer personas representing key Indian segments", "Prioritize segments using pain intensity, willingness to pay, and accessibility criteria"]'::jsonb,
    '{"templates": ["India Customer Segment Matrix", "Persona Template with Indian Demographics", "Segment Sizing Calculator India", "Segment Prioritization Scorecard"], "tools": ["NSSO Data Analysis Guide", "India Income Distribution Reference", "Regional Language User Mapping", "Digital Adoption Curve by Tier"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_1_id, 4, 'Synthesizing Customer Insights from Indian Research',
    'Transform raw interview data into actionable insights while accounting for Indian communication patterns. Indian interviewees often provide aspirational rather than actual responses - learn to identify this gap. Look for behavior indicators: what they actually do matters more than what they say they want. Cross-reference stated preferences with actual spending patterns. Use affinity mapping to identify patterns across interviews - cluster insights by theme, intensity, and frequency. Indian-specific insight categories to capture: price sensitivity thresholds (where does the purchase decision change?), trust signals required (reviews, certifications, celebrity endorsements, word-of-mouth), family influence patterns (who influences, who decides, who pays), and regional variations (does the insight hold across geographies?). Create insight statements that drive product decisions: "Urban working mothers in Bangalore are willing to pay 2x for grocery delivery that saves 2+ hours weekly because they value time with children more than money saved." Validate insights quantitatively: use Google Forms surveys distributed via WhatsApp for validation at scale. Sample size guidance: 200+ responses for directional insights, 500+ for statistical significance in a single segment. Build your customer evidence repository: organize by insight theme, include direct quotes (anonymized), note regional and demographic context, and tag by product implication.',
    '["Conduct 15 customer discovery interviews across Metro, Tier 1, and Tier 2 cities", "Create affinity map identifying patterns in Indian customer needs and behaviors", "Write 10 key insight statements with supporting evidence and regional validation", "Design and distribute validation survey via WhatsApp to 200+ potential customers"]'::jsonb,
    '{"templates": ["Interview Note Template India", "Affinity Mapping Canvas", "Insight Statement Framework", "WhatsApp Survey Distribution Guide"], "tools": ["Insight Synthesis Worksheet", "Evidence Repository Structure", "Regional Pattern Analyzer", "Survey Analysis Dashboard"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_1_id, 5, 'Problem Validation Scorecard for Indian Market',
    'Evaluate problem-solution fit quantitatively with India-specific validation criteria. Indian market validation requires higher evidence threshold due to the aspirational response bias. Problem validation scorecard dimensions: Problem Intensity - how painful is this problem on 1-10 scale (threshold: average >7), Problem Frequency - how often does this problem occur (threshold: weekly or more for consumer, monthly for B2B), Current Solutions - how are customers solving this today and satisfaction level (threshold: <5/10 satisfaction), Willingness to Pay - would they pay and how much (threshold: >50% would pay, at price point supporting unit economics), Problem Urgency - is this a burning problem or nice-to-have (threshold: >30% describe as urgent). India-specific validation signals: customers sharing the problem unprompted, customers introducing you to others with the same problem, customers willing to pay deposit/advance, customers already spending money on inadequate alternatives. Red flags for Indian market: everyone likes the idea but no one commits, price sensitivity only discussion point, heavy reliance on discounts to generate interest, and regional inconsistency in problem intensity. Set go/no-go thresholds: Green (proceed to MVP) requires 8+ of 10 validation criteria met, Yellow (iterate on problem) requires 5-7 criteria met, Red (pivot or kill) means fewer than 5 criteria met. Document pivot triggers: what would need to be true to abandon this problem space? Create learning log for each customer segment tested.',
    '["Complete problem validation scorecard for each target segment", "Calculate problem intensity, frequency, and urgency metrics from interviews", "Test willingness to pay through advance payment or deposit experiments", "Set explicit go/no-go thresholds and document pivot decision criteria"]'::jsonb,
    '{"templates": ["Problem Validation Scorecard India", "Go/No-Go Decision Framework", "Pivot Trigger Documentation", "Learning Log Template"], "tools": ["Validation Metrics Calculator", "WTP Testing Guide", "Segment Comparison Matrix", "Pivot Decision Tree"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 2: Problem Validation (Days 6-10)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_2_id, v_product_id, 'Problem Validation', 'Validate market size, competitive landscape, and timing for Indian market entry with TAM/SAM/SOM analysis', 2, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_2_id, 6, 'TAM SAM SOM Analysis for Indian Markets',
    'Market sizing in India requires understanding the unique market structure with massive population but concentrated spending power. India market characteristics: 1.4 billion population but effective consumer market is 300-400 million with discretionary spending power. Top-down market sizing approach: Start with global market size, apply India GDP ratio (3.5% of global), adjust for purchasing power parity (PPP multiplier of 3-4x), and account for formalization rate (organized market is 30-50% of total in most categories). Bottom-up approach (more reliable for India): identify target segment population, multiply by problem frequency, multiply by willingness to pay, and apply realistic adoption rate. Indian market sizing benchmarks: Total Addressable Market (TAM) should be >$1B for venture-scale opportunity in India; Serviceable Addressable Market (SAM) typically 5-15% of TAM based on geographic and segment focus; Serviceable Obtainable Market (SOM) is realistic 1-3 year capture, typically 1-5% of SAM for new entrants. Data sources for Indian market sizing: IBEF industry reports (free), RedSeer consulting reports, Statista India data, NSSO consumption surveys, RBI economic data, and industry association reports. Common mistakes: using US/global data without India adjustment, ignoring the informal economy, overestimating digital adoption in Tier 2+, and not accounting for price sensitivity reducing market value. Present market size in both volume (users/transactions) and value (revenue) - Indian VCs often focus on volume potential given value appreciation over time.',
    '["Calculate TAM using top-down approach with India-specific multipliers", "Validate with bottom-up calculation from target segment sizing", "Define SAM with realistic geographic and segment scope for Year 1-3", "Create investor-ready market sizing slides with sourced data points"]'::jsonb,
    '{"templates": ["TAM SAM SOM Calculator India", "Market Sizing Methodology Document", "Data Source Reference Guide", "Investor Market Slide Template"], "tools": ["India GDP/PPP Converter", "Segment Population Calculator", "Market Sizing Validation Checklist", "Industry Report Aggregator"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_2_id, 7, 'Competitive Landscape Mapping in India',
    'Indian startup ecosystem has 90,000+ startups creating intense competition in most categories. Competitive analysis framework for India: Direct competitors (same product/service, same customer), Indirect competitors (different product, same job-to-be-done), Substitute solutions (traditional/offline alternatives), and Future competitors (well-funded startups that could pivot). India-specific competitive dynamics: "Me-too" culture means successful models get copied within 6 months by 10+ players, price wars are common and funded players can sustain losses for years, regional players may have strong local advantages, and incumbents (traditional businesses) often have distribution and trust advantages. Analyze competitors across dimensions: Product features and quality, Pricing and business model, Distribution and reach (online vs offline, geographic coverage), Marketing and brand perception, Team and funding (indicates runway and resources), and Customer experience and retention. Information sources for competitor research: LinkedIn for team size and hiring patterns, Crunchbase/Tracxn for funding history, SimilarWeb for traffic estimates, App Annie for app downloads, Google Trends for search interest, social media for customer feedback, and Glassdoor for employee insights. Create competitive positioning map: plot on 2x2 matrix using dimensions most important to customers (e.g., price vs quality, features vs simplicity). Indian consumer willingness to switch: 40% will switch for 10% price difference, 60% for 20% difference, but loyalty programs and trust can reduce switching.',
    '["Map all direct, indirect, substitute, and potential future competitors", "Create feature and pricing comparison matrix with 10+ competitors", "Build competitive intelligence dashboard tracking funding, team, and marketing", "Develop 2x2 positioning map identifying your differentiation opportunity"]'::jsonb,
    '{"templates": ["Competitive Analysis Matrix India", "Feature Comparison Template", "Competitor Intelligence Dashboard", "Positioning Map Canvas"], "tools": ["Competitor Discovery Checklist", "Funding Tracker Setup", "Social Listening Keywords", "Competitive Response Framework"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_2_id, 8, 'Finding Your Competitive Moat in India',
    'Sustainable competitive advantages in India require understanding what moats are buildable in this market. Types of competitive moats applicable to India: Network effects (strongest moat - marketplace dynamics, community effects, data network effects), Switching costs (integration depth, data lock-in, habit formation, relationship investment), Economies of scale (cost advantages from volume, geographic density economics), Brand (trust premium, status association, category ownership), Proprietary technology (patents less enforceable in India, but speed and execution matter), and Regulatory advantages (licenses, compliance capabilities, government relationships). India-specific moat considerations: Trust is the ultimate moat - Indian consumers heavily weigh reputation and track record. Regional density moats - being the dominant player in specific geography often beats being marginal everywhere. Vernacular moats - deep regional language capabilities are difficult to replicate. Relationship moats - B2B success in India is relationship-driven, not just product-driven. Moat strength in Indian context: network effects can be powerful but require critical mass (which requires funding), brand moats are buildable but require sustained marketing investment, switching costs work but Indian consumers are price-sensitive and will switch for savings, and technology moats erode faster due to talent availability and copying culture. Build moat strategy: identify which moat types are realistic for your business stage, sequence moat-building activities, and invest early in trust-building even before revenue.',
    '["Evaluate 7 moat types for feasibility in your specific business and market", "Identify 2-3 strongest moat opportunities with realistic build timeline", "Create moat-building activity roadmap for next 12-18 months", "Document trust-building strategy as foundational Indian market moat"]'::jsonb,
    '{"templates": ["Competitive Moat Assessment India", "Moat Building Roadmap", "Trust Building Strategy Canvas", "Network Effects Analysis"], "tools": ["Moat Strength Evaluator", "Switching Cost Calculator", "Brand Moat Assessment", "Regional Density Analyzer"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_2_id, 9, 'Market Timing Analysis for India',
    'Why now is the right time matters as much as what you are building. India timing factors framework: Technology enablers (what new technology makes this possible now?), Infrastructure readiness (digital payments, logistics, internet penetration), Regulatory changes (new policies, liberalization, compliance requirements), Demographic shifts (income growth, urbanization, education levels), and Behavioral changes (adoption patterns, digital comfort, trust in new solutions). Recent India macro trends enabling new opportunities: UPI reaching 10+ billion monthly transactions making digital payments universal, 5G rollout enabling new video/AR experiences, working from home normalizing digital tools adoption, quick commerce setting expectations for instant delivery, and vernacular internet users exceeding 500 million. Regulatory tailwinds: ONDC for e-commerce democratization, Account Aggregator for fintech, National Digital Health Mission for healthtech, NEP 2020 for edtech, and Production Linked Incentive schemes for manufacturing. Technology adoption lifecycle in India: Innovators (2-3%) are metro, English-speaking, high-income; Early Adopters (10-15%) are urban, digitally comfortable; Early Majority (35%) need social proof and trust signals; Late Majority (35%) need convenience and price; Laggards (15%) resist until absolutely necessary. Timing risks: premature if infrastructure not ready, late if competition already established. Document "why now" thesis with specific evidence points.',
    '["List 10 specific why-now factors enabling your opportunity in current India", "Map your solution to technology adoption lifecycle stage by segment", "Identify regulatory tailwinds and headwinds affecting your market", "Create compelling why-now narrative for investor and team communication"]'::jsonb,
    '{"templates": ["Why Now Framework India", "Market Timing Canvas", "Regulatory Impact Analysis", "Adoption Curve Mapping"], "tools": ["India Macro Trends Tracker", "Technology Adoption Calculator", "Regulatory Change Monitor", "Timing Risk Assessment"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_2_id, 10, 'Problem Validation Synthesis',
    'Combine customer discovery and market analysis into comprehensive validation thesis. Problem validation synthesis framework: Problem clarity (do we deeply understand the problem?), Customer clarity (do we know who has this problem most acutely?), Market validation (is the market large enough and growing?), Competitive validation (can we win against alternatives?), and Timing validation (is now the right time?). Create problem validation document structure: Executive summary of problem and opportunity, Customer insight synthesis with segment-specific findings, Market size analysis with methodology and sources, Competitive landscape with differentiation thesis, Timing thesis with enabling factors, and Key risks and mitigation strategies. Validation confidence scoring: High confidence (70%+ validation criteria met with quantitative evidence), Medium confidence (50-70% met, some qualitative-only evidence), Low confidence (<50% met, requires more research). Decision framework: High confidence + large market = proceed aggressively, High confidence + small market = consider niche strategy or pivot to larger market, Medium confidence = continue validation before major investment, Low confidence = pivot problem space or customer segment. Indian investor expectations: Indian VCs are pattern-matchers looking for proven models; validation should include comparisons to successful Indian and global examples. Pre-seed validation milestone: 50+ customer interviews, quantified problem intensity, validated WTP, clear segment definition, and competitive differentiation thesis.',
    '["Create comprehensive problem validation document synthesizing all research", "Complete validation confidence scoring across all five dimensions", "Prepare problem validation presentation for stakeholders and investors", "Document key risks, assumptions, and next validation milestones"]'::jsonb,
    '{"templates": ["Problem Validation Document Template", "Validation Confidence Scorecard", "Investor Problem Validation Deck", "Risk and Assumption Log"], "tools": ["Synthesis Checklist", "Confidence Score Calculator", "Validation Milestone Tracker", "Decision Framework Guide"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 3: MVP Design (Days 11-15)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_3_id, v_product_id, 'MVP Design', 'Design Minimum Viable Products optimized for Indian users with feature prioritization, prototyping, and validation strategies', 3, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_3_id, 11, 'Feature Prioritization for Indian Products',
    'Feature prioritization in India must balance aspiration with accessibility. Indian users have high feature expectations set by global apps but low tolerance for complexity. Prioritization frameworks for India: RICE adapted (Reach in India segments, Impact on Indian user problems, Confidence with Indian user validation, Effort considering Indian dev costs), MoSCoW with Indian lens (Must-haves are trust and basic function, Should-haves are Indian payment and language support, Could-haves are delight features, Won''t-haves are Western-centric features). Indian market must-have features: UPI and COD payment support (non-negotiable for consumer apps), WhatsApp integration for support and updates, Hindi/vernacular language option for Tier 2+ reach, lightweight app version for low-end devices, and offline capability for patchy connectivity. Feature prioritization for Indian price sensitivity: every feature has cost - prioritize features that justify price premium, avoid over-engineering that raises costs without value, and consider freemium with paid vernacular/premium features. India-specific feature tradeoffs: depth vs breadth (Indian users prefer comprehensive solutions), polish vs speed (launch fast, iterate based on feedback), global best practices vs Indian adaptations (localize rather than copy), and feature parity vs differentiation (differentiate on what matters locally). Data-informed prioritization: use WhatsApp polls for quick feature validation, Google Forms for detailed preference ranking, and fake door tests to measure feature demand before building.',
    '["List all potential features and score using RICE framework with India adjustments", "Apply MoSCoW categorization with Indian market must-haves identified", "Identify features for Indian accessibility: UPI, vernacular, lightweight, offline", "Create prioritized feature backlog with clear rationale and validation status"]'::jsonb,
    '{"templates": ["RICE Scoring Template India", "MoSCoW Prioritization Matrix", "India Must-Have Feature Checklist", "Feature Backlog Template"], "tools": ["Feature Prioritization Calculator", "WhatsApp Poll Creator Guide", "Fake Door Test Setup", "Feature Validation Tracker"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_3_id, 12, 'Defining MVP Scope for Indian Market',
    'The true MVP in India must balance minimum functionality with minimum trust signals. Indian users are skeptical of new products - MVP must prove legitimacy, not just functionality. MVP definition for India: the smallest product that can validate your core value proposition while meeting Indian user trust thresholds. Trust-minimum requirements for Indian MVP: professional-looking design (even if limited functionality), clear company information and contact details, secure payment with recognizable gateway, return/refund policy clearly stated, and social proof elements (even if just founder credibility). Core value hypothesis: identify the single most important promise your product makes, the one job-to-be-done it must excel at. MVP scope discipline: every feature not directly testing your core hypothesis is a distraction. "Good enough" threshold in India: functionality can be 70% complete but trust signals must be 100% complete. MVP anti-patterns in India: overbuilding due to competitive fear, adding features based on loud customer requests rather than data, and polishing non-core features. Set MVP success criteria: define specific metrics that prove/disprove your hypothesis. Typical MVP success metrics for India: conversion rate (2-5% for consumer, 10-20% for B2B), retention (D7 30%+ for consumer apps), NPS (>30 for India given lower baseline), and willingness to pay (>20% convert to paid). Timeline guidance: consumer MVP in 4-8 weeks, B2B MVP in 8-12 weeks - longer timelines indicate scope creep.',
    '["Define core value hypothesis in single clear statement", "Identify minimum feature set to test core hypothesis", "List trust signals required for Indian user acceptance", "Set 3-5 measurable MVP success criteria with specific thresholds"]'::jsonb,
    '{"templates": ["MVP Scope Definition Canvas", "Core Hypothesis Statement Template", "Trust Signal Checklist India", "MVP Success Criteria Framework"], "tools": ["MVP Scope Validator", "Feature Cut Decision Guide", "Success Metric Calculator", "Timeline Estimator"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_3_id, 13, 'Prototype Tools and Techniques for India',
    'Prototyping in India benefits from strong design talent pool and cost-effective tools. Prototype fidelity selection: Paper prototypes for initial concept validation (cost: minimal), Wireframes for flow and structure testing (tools: Balsamiq, Whimsical), Medium-fidelity for user testing with realistic interactions (tools: Figma, Adobe XD), High-fidelity for final validation and investor demos (tools: Figma with prototyping), and Coded prototypes for complex interactions or technical validation. No-code tools popular in India for MVP building: Bubble for web apps (learning curve but powerful), Glide for mobile apps from Google Sheets, Webflow for marketing websites and simple apps, Adalo for native mobile apps, and Softr/Stacker for internal tools. Indian no-code developer costs: ₹25,000-50,000 for simple app, ₹50,000-1,50,000 for complex app - significantly cheaper than coded development. Prototype for Indian users: design for thumb-zone navigation (Indian users hold phones differently), use larger tap targets (smaller screens, imprecise touch), ensure text readability in sunlight (outdoor usage common), and test with vernacular text (different character widths). Prototype validation in India: conduct testing sessions via Google Meet (widely accessible), use WhatsApp screen recording sharing, leverage UserTesting India panel, and pay participants ₹200-500 for 30-minute sessions. Document what prototype will specifically test: each prototype should have clear hypothesis and success criteria.',
    '["Select appropriate prototype fidelity based on validation goals and timeline", "Choose prototyping tools considering team skills and testing needs", "Create clickable prototype of core user flow optimized for Indian mobile usage", "Design prototype test plan with specific hypotheses to validate"]'::jsonb,
    '{"templates": ["Prototype Fidelity Selection Guide", "Tool Selection Matrix India", "Indian Mobile Design Checklist", "Prototype Test Plan Template"], "tools": ["Figma India Components Library", "No-Code Tool Comparison", "User Testing Recruitment Guide", "Prototype Feedback Collector"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_3_id, 14, 'MVP Types and Selection for Indian Context',
    'Different MVP approaches work differently in Indian market conditions. MVP types and Indian applicability: Wizard of Oz (human-powered backend) - excellent for India given low labor costs, can validate without building; Concierge (high-touch manual service) - works well for premium segments, builds relationships and insights; Landing Page (gauge demand before building) - effective for measuring Indian market interest, use regional language variants; Single Feature (one thing done well) - focus on core job-to-be-done, avoid feature creep; Piecemeal (existing tools stitched together) - combine WhatsApp, Google Sheets, Razorpay for functional MVP; and Crowdfunded (validate demand through pre-orders) - moderate success in India, works for aspirational products. Indian market MVP considerations: Wizard of Oz works exceptionally well - operations talent is affordable, service quality can be controlled, and you learn deeply about customer needs. WhatsApp-first MVPs are uniquely effective in India - customers are comfortable, no app download friction, and high engagement. Indian B2B MVP pattern: start with consulting/services to learn the problem, then productize the solution. MVP type selection criteria: speed to market, learning depth, cost to build, technical risk, and ability to iterate. Common mistake: building technology MVP when manual MVP would provide same learnings faster and cheaper.',
    '["Evaluate 6 MVP types against your specific product and market context", "Select primary MVP approach with clear rationale", "Design MVP experiment with specific hypotheses and success metrics", "Create MVP development timeline with weekly milestones"]'::jsonb,
    '{"templates": ["MVP Type Evaluation Matrix", "Wizard of Oz Design Template", "WhatsApp MVP Blueprint", "MVP Experiment Canvas"], "tools": ["MVP Type Decision Tree", "Manual MVP Cost Calculator", "Learning Velocity Estimator", "MVP Timeline Planner"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_3_id, 15, 'MVP Launch Strategy for India',
    'MVP launch in India requires balancing speed with trust-building. Pre-launch preparation: set up basic legal entity (Pvt Ltd gives more credibility than proprietorship), create professional social media presence, prepare customer support workflow (WhatsApp is expected), and have payment gateway ready (Razorpay most trusted). Launch audience selection for India: start with metropolitan early adopters (more forgiving of bugs), focus on specific geography initially (Bangalore tech, Mumbai business, Delhi aspirational), leverage founder networks for initial users, and use WhatsApp groups for community building. Launch channels in India: Product Hunt India for tech products, WhatsApp broadcast lists for direct audience, Instagram for consumer products, LinkedIn for B2B, and Twitter for tech/startup community. Feedback collection setup: in-app feedback widget (Hotjar, Canny), WhatsApp feedback group with active users, Google Form for structured feedback, and NPS survey at key moments. Analytics setup for MVP: implement Mixpanel or Amplitude for event tracking, set up funnel tracking from signup to activation to retention, track by segment (geography, language, acquisition source), and define leading indicators of success. Iteration cadence: plan for weekly releases based on feedback, prioritize based on impact and effort, communicate changes to users (builds trust), and celebrate improvements publicly.',
    '["Complete pre-launch checklist including legal, payments, and support setup", "Define launch audience criteria and acquisition plan for first 100 users", "Set up analytics tracking for all key user events and conversion points", "Create 4-week post-launch iteration plan with weekly release targets"]'::jsonb,
    '{"templates": ["MVP Launch Checklist India", "Launch Audience Definition", "Analytics Implementation Guide", "Post-Launch Iteration Plan"], "tools": ["Razorpay Integration Guide", "WhatsApp Support Setup", "Mixpanel Event Tracking Template", "Weekly Release Planning Tool"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 4: Prototyping (Days 16-20)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_4_id, v_product_id, 'Prototyping', 'Build and iterate prototypes optimized for Indian users with low-bandwidth considerations and vernacular design', 4, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_4_id, 16, 'Low-Bandwidth Product Design for India',
    '500 million Indians access internet on sub-3G speeds or congested networks. India connectivity reality: average mobile speed 15-25 Mbps but highly variable, 4G coverage at 85% but actual speeds often 3G-level, rural areas on 2G/3G, and data costs matter (users track MB usage). Low-bandwidth design principles: lazy loading (load content as needed), image optimization (WebP format, appropriate sizing), offline-first architecture (local storage, background sync), progressive enhancement (core functionality works on slow connections), and aggressive caching (reduce repeated downloads). Performance benchmarks for India: First Contentful Paint under 1.5 seconds on 3G, Time to Interactive under 3 seconds, total page weight under 500KB, and app size under 30MB (impacts downloads). Technical implementation: use CDN with Indian PoPs (Cloudflare free tier has Mumbai, Chennai), implement service workers for offline capability, compress images to under 50KB for thumbnails, use skeleton loaders for perceived performance, and minimize JavaScript bundle size. Data-saving features Indian users love: data saver mode toggle, image quality settings, offline mode for key features, and download management (save for later). Testing for Indian conditions: use Chrome DevTools network throttling (3G preset), test on mid-range Android devices (Redmi, Realme), and use real device testing services with India infrastructure.',
    '["Audit current product for performance on 3G network conditions", "Implement image optimization with WebP and lazy loading", "Add offline capability for core user flows", "Create data saver mode with user-controllable settings"]'::jsonb,
    '{"templates": ["Low-Bandwidth Design Checklist", "Performance Budget Template", "Offline-First Architecture Guide", "Data Saver Feature Spec"], "tools": ["PageSpeed Insights Setup", "Lighthouse CI Configuration", "Image Compression Pipeline", "CDN Setup Guide India"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_4_id, 17, 'Vernacular Product Design',
    'Only 12% of Indians prefer English-first digital experiences. Hindi is spoken by 44% but regional languages dominate in their respective states. Language landscape: Hindi (500M speakers), Bengali (100M), Telugu (85M), Marathi (85M), Tamil (75M), Gujarati (55M), Kannada (45M), Malayalam (35M), Odia (35M), Punjabi (30M). Vernacular design considerations: text expansion (Hindi is 25-30% longer than English), right-to-left considerations for Urdu, script-specific font requirements, and transliteration vs translation (many users read Hindi in English script). Localization levels: Level 1 (interface translation) - buttons, labels, navigation; Level 2 (content translation) - product descriptions, help content; Level 3 (cultural localization) - imagery, examples, references; Level 4 (regional adaptation) - pricing, offerings, features. Implementation approach: start with Hindi for maximum reach, add regional languages based on user base, use Google Translate API for initial translation then human review, and partner with language experts for cultural nuance. Vernacular content management: plan for multi-language content updates from day one, use content management system with language support, and budget 30-40% more for content creation in multiple languages. Vernacular UX patterns: audio support for non-literate users (30% of India), visual instructions over text, and familiar metaphors from Indian context.',
    '["Create vernacular strategy defining languages to support and timeline", "Design UI components that accommodate text expansion and different scripts", "Implement language selection with easy switching", "Partner with regional language content creators for authentic localization"]'::jsonb,
    '{"templates": ["Vernacular Strategy Framework", "Multi-Language UI Design Guide", "Localization Content Brief", "Regional Language Style Guide"], "tools": ["Translation Memory Setup", "Language Detection Implementation", "Font Loading Optimization", "Vernacular Testing Protocol"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_4_id, 18, 'Mobile-First Design for Indian Users',
    'India is mobile-first with 85% accessing internet via smartphones. 75% of these are Android devices, predominantly mid-range. Indian smartphone usage patterns: primary device for all digital activity, shared device in many households (design for multi-user), smaller screen sizes (5.5-6.5 inch average), and aggressive app deletion due to storage constraints. Android-first design priorities: work on Android 8+ (covers 90% of Indian Android users), test on mid-range devices (2-4GB RAM, MediaTek/Snapdragon 600 series), optimize memory usage and battery consumption, and support both portrait and landscape for entertainment apps. Touch target design: minimum 44x44 pixels, adequate spacing between targets, thumb-zone optimization for one-handed use, and gesture support for navigation. Screen size considerations: design for 360px width as baseline, use relative sizing not fixed pixels, and test across screen densities. Indian mobile UX patterns: bottom navigation preferred over hamburger menu, prominent search always accessible, pull-to-refresh expected, and in-app browser for external links (avoid kicking to browser). Mobile form design: minimize typing (auto-suggestions, phone auth), support autocomplete, use numeric keypads for numbers, and show progress for multi-step forms. App size and performance: target under 30MB APK size, implement on-demand feature loading, monitor and optimize battery usage, and reduce background data consumption.',
    '["Audit app design for mobile-first Indian users on mid-range Android", "Optimize touch targets and navigation for one-handed use", "Implement thumb-zone analysis and adjust key action placements", "Test on target devices (Redmi Note series, Realme, Samsung A series)"]'::jsonb,
    '{"templates": ["Mobile-First Design Audit Checklist", "Thumb Zone Layout Guide", "Android Optimization Checklist", "Device Testing Matrix India"], "tools": ["Touch Target Analyzer", "Screen Size Simulator", "Battery Usage Profiler", "APK Size Optimizer"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_4_id, 19, 'Trust-Building Design Elements',
    'Trust is the primary barrier to adoption for new products in India. Trust deficit reality: 60% of Indians have experienced online fraud or know someone who has, new brands face skepticism, and unfamiliar payment flows cause abandonment. Visual trust signals: professional design quality (no typos, consistent branding), clear company information (registered address, GSTIN), secure payment badges (PCI DSS, RBI regulated), and recognized certification logos. Social proof elements: customer reviews with photos and names, verifiable customer count (be honest), media mentions and logos, and testimonials from recognizable figures or companies. Transaction trust: familiar payment options (UPI, all major wallets), COD option prominently displayed, clear return and refund policy, and order tracking with real-time updates. Communication trust: responsive customer support (WhatsApp expected), timely order and delivery updates, proactive issue resolution, and human touch in automated communication. Trust through transparency: clear pricing without hidden charges, honest delivery timelines, product descriptions matching reality, and acknowledgment of limitations. Trust building for B2B: company registration details, case studies with named clients, team profiles with LinkedIn links, and physical office address. Anti-fraud signals: clear cancellation process, no pressure tactics, privacy policy in simple language, and data usage transparency.',
    '["Audit product for trust signals at every user touchpoint", "Add visible security and compliance certifications", "Implement social proof elements authentically", "Create transparent policies (returns, refunds, privacy) in simple Hindi and English"]'::jsonb,
    '{"templates": ["Trust Audit Checklist India", "Social Proof Implementation Guide", "Policy Template Library (India)", "Trust Signal Design System"], "tools": ["Trust Score Assessment", "Review Collection System", "Certification Badge Library", "Customer Support Setup Guide"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_4_id, 20, 'Rapid Prototyping and Iteration',
    'Speed of iteration is a competitive advantage in the fast-moving Indian market. Rapid prototyping mindset: ship fast, learn fast, iterate fast. Perfect is the enemy of good enough. Indian market moves quickly - first learnings beat first features. Prototype iteration cycle for India: Week 1 - identify top 3 user pain points from feedback, Week 2 - design and build solutions, Week 3 - release and measure, Week 4 - analyze and plan next iteration. Feedback collection at scale: in-app NPS at key moments, WhatsApp feedback groups with power users, app store reviews (respond to all), and social media mentions and DMs. Prioritization framework for iterations: fix what causes user drop-off first, add what users actively request (with validation), improve what increases key metrics, and remove what nobody uses. A/B testing in India: tools like Firebase Remote Config, Statsig, or PostHog, test one variable at a time, require 1000+ users per variant for significance, and account for regional variation in results. Iteration communication: share updates publicly (builds trust), acknowledge user feedback in release notes, celebrate user-requested features, and be transparent about roadmap. Iteration velocity benchmarks: consumer apps should ship weekly, B2B products bi-weekly, and major features monthly. Documentation: maintain decision log for why changes were made, track feature success metrics, and document failed experiments for learning.',
    '["Establish weekly iteration cycle with clear process and roles", "Set up feedback collection across all channels (in-app, WhatsApp, reviews)", "Implement A/B testing infrastructure for data-driven decisions", "Create public changelog and feedback acknowledgment process"]'::jsonb,
    '{"templates": ["Weekly Iteration Checklist", "Feedback Synthesis Template", "A/B Test Design Document", "Release Notes Template"], "tools": ["Firebase Remote Config Setup", "User Feedback Dashboard", "Feature Success Metrics Tracker", "Decision Log Format"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 5: User Testing (Days 21-25)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_5_id, v_product_id, 'User Testing', 'Conduct effective user testing with Indian users across metros and tier cities', 5, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_5_id, 21, 'User Testing Fundamentals for India',
    'User testing in India requires adapting methodologies for cultural context. User testing types: moderated testing (facilitator guides user through tasks), unmoderated testing (user completes tasks independently), A/B testing (comparing two versions), and diary studies (long-term behavior tracking). Indian user testing challenges: users tend to give positive feedback to avoid conflict, language barriers for non-English speakers, digital literacy variation, and participants may not verbalize thought process naturally. Moderated testing adaptations for India: build rapport before testing (chai conversation), ask indirect questions ("what would your friend think?"), observe behavior more than stated preference, and allow family members if that reflects real usage. Remote testing setup: use Google Meet (widely accessible), have participant share screen via phone, record with permission, and provide clear joining instructions in Hindi/English. Sample size guidance: 5-8 users per segment for qualitative insights, 100+ for quantitative validation, include representation across tier cities and age groups. Participant recruitment in India: social media posts with screening survey, existing user database, referral from participants, and paid panel services (UserTesting India, PlaybookUX). Compensation: Metro participants ₹500-1000 per hour, Tier 2/3 participants ₹300-500, consider mobile recharge or e-commerce vouchers.',
    '["Design user testing protocol adapted for Indian cultural context", "Create screening criteria to recruit diverse participants across tiers and segments", "Set up remote testing infrastructure (Meet, screen recording, consent forms)", "Build participant database with recruitment channels identified"]'::jsonb,
    '{"templates": ["India User Testing Protocol", "Participant Screener Questionnaire", "Remote Testing Setup Checklist", "Consent Form Template (Hindi/English)"], "tools": ["Participant Recruitment Tracker", "Session Recording Setup Guide", "Compensation Guidelines India", "Testing Calendar Planner"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_5_id, 22, 'Designing Effective User Tests',
    'Well-designed user tests yield actionable insights for Indian product development. Test design components: clear objectives (what are you trying to learn?), task scenarios (realistic user situations), success criteria (how will you measure?), and script (what to say and observe). Task scenario design for India: use realistic Indian contexts (booking for family, finding deals, time-saving), include typical constraints (budget limits, family approval needs), and account for regional scenarios. Task types: directed tasks ("find and purchase a specific item"), exploratory tasks ("explore the app and tell me what you think"), and comparative tasks ("complete this using our app vs competitor"). Script structure: introduction and rapport building (5 min), warm-up questions about current behavior (5 min), core tasks with think-aloud protocol (30 min), follow-up probing questions (10 min), and wrap-up and thank you (5 min). Think-aloud adaptation for India: many users uncomfortable verbalizing, prime them with examples, ask "what are you looking at now?" frequently, and observe hesitations and confusion without verbal cue. Metrics to capture: task completion rate, time on task, error rate, satisfaction rating (1-5), and specific usability issues. Avoid leading questions: instead of "do you like this?", ask "what do you think about this?" or "how would you describe this to a friend?"',
    '["Define clear test objectives with specific questions to answer", "Create 5-7 realistic task scenarios for Indian user contexts", "Write test script with introduction, tasks, and follow-up questions", "Design measurement framework with success criteria for each task"]'::jsonb,
    '{"templates": ["Test Objectives Framework", "Task Scenario Template India", "User Test Script Template", "Metrics Capture Sheet"], "tools": ["Task Scenario Generator", "Success Criteria Calculator", "Script Checklist", "Note-Taking Template"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_5_id, 23, 'Conducting User Testing Sessions',
    'Effective facilitation extracts genuine insights from Indian users. Pre-session preparation: test all technology (Meet link, screen sharing, recording), have backup plan for tech issues, prepare participant info packet, and send reminders via WhatsApp (higher open rate). Session facilitation tips for India: start with personal conversation (build trust), explain there are no right/wrong answers, emphasize you are testing the product not them, and speak in their preferred language if possible. During the session: stay neutral (no reactions to user actions), use silence productively (let user think), probe deeper on interesting behaviors ("tell me more about that"), and note body language and hesitations. Common challenges with Indian participants: tendency to ask facilitator for help (redirect: "what would you do if I wasn't here?"), giving expected answers (probe for actual behavior), going off-topic (gently redirect), and tech difficulties (have troubleshooting ready). Note-taking during sessions: capture direct quotes, note timestamp of interesting moments, record observed vs stated behavior differences, and flag critical usability issues. Post-session debrief: review recording immediately, note top insights while fresh, identify clips for sharing with team, and update findings document.',
    '["Complete pre-session checklist for first 5 user testing sessions", "Conduct sessions using adapted facilitation techniques", "Create note-taking system capturing quotes, behaviors, and issues", "Perform immediate post-session debriefs with recorded insights"]'::jsonb,
    '{"templates": ["Pre-Session Checklist", "Facilitation Guide India", "Session Note Template", "Debrief Worksheet"], "tools": ["Technology Troubleshooting Guide", "Recording Clip Extractor", "Quote Capture System", "Insight Flagging Tool"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_5_id, 24, 'Analyzing User Testing Results',
    'Transform raw testing data into actionable product improvements. Analysis framework: quantitative analysis (task success rates, time on task, error rates) and qualitative analysis (themes, pain points, user quotes). Quantitative thresholds for India: Task success rate >80% acceptable, 60-80% needs improvement, <60% critical issue. Time on task within 1.5x expected is acceptable. Error rate <10% acceptable, >20% critical. Qualitative analysis process: review all session recordings, create affinity map of issues by theme, count frequency of each issue, rate severity (critical, major, minor, cosmetic), and identify patterns across user segments. Severity rating for Indian context: Critical - prevents task completion or causes data/money loss, Major - significant difficulty or workaround required, Minor - noticeable issue but task can be completed, Cosmetic - minor visual or text issues. Issue prioritization: map issues on severity vs frequency matrix, prioritize high-severity high-frequency first, consider quick wins (low effort, high impact). Insight synthesis: create issue summary with evidence (quotes, clips), recommendations for each issue, and impact assessment. Share findings: create video highlights reel for stakeholders, executive summary with top 5-10 issues, and detailed findings document for product team.',
    '["Calculate quantitative metrics from all testing sessions", "Create affinity map grouping issues by theme and frequency", "Rate all issues by severity using India-adapted criteria", "Prioritize issues using severity vs frequency matrix"]'::jsonb,
    '{"templates": ["Quantitative Analysis Template", "Affinity Mapping Canvas", "Severity Rating Guide India", "Issue Prioritization Matrix"], "tools": ["Metrics Calculator", "Theme Identification Guide", "Video Highlights Creator", "Findings Report Template"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_5_id, 25, 'Acting on User Testing Insights',
    'Convert user testing insights into measurable product improvements. From insights to action: translate each insight into specific product change, estimate effort required, map to existing roadmap, and assign ownership. Quick wins: implement easy fixes immediately (typos, label clarity, button sizing), communicate changes to participants (builds relationship), and measure impact of changes. Major changes: create detailed requirements from insights, include user quotes and clips in specs, design solution with multiple options, and test solution before full implementation. Stakeholder communication: share findings presentation with key clips, frame issues in business impact terms (conversion loss, support costs), recommend prioritization, and get alignment on action plan. Closing the loop: inform participants of changes made based on their feedback, build ongoing relationship with engaged testers, create user advisory panel for future testing. Continuous testing rhythm: establish regular testing cadence (monthly for active development), rotate participant pool, test specific features before launch, and conduct regression testing on core flows. Track impact: measure before and after metrics for changed features, A/B test significant changes, and document improvement trends over time.',
    '["Create action plan mapping each insight to specific product changes", "Implement quick wins within one week of testing completion", "Present findings to stakeholders with business impact framing", "Establish ongoing user testing rhythm with monthly cadence"]'::jsonb,
    '{"templates": ["Insight to Action Plan", "Quick Win Implementation Tracker", "Stakeholder Presentation Template", "Testing Rhythm Calendar"], "tools": ["Change Impact Calculator", "Before/After Comparison Framework", "User Advisory Panel Setup", "Improvement Tracking Dashboard"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 6: Agile Development (Days 26-30)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_6_id, v_product_id, 'Agile Development', 'Implement agile methodologies adapted for Indian startup teams and development contexts', 6, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_6_id, 26, 'Agile Fundamentals for Indian Startups',
    'Agile methodologies need adaptation for Indian startup contexts and team dynamics. Indian startup team characteristics: often remote or hybrid teams across multiple cities, hierarchical culture influences communication styles, Indian holidays and festival seasons impact sprints, and talent retention challenges require team-centric practices. Agile principles relevant for India: individuals and interactions over processes (but documentation helps with team turnover), working software over comprehensive documentation (but Indian developers often prefer clear specs), customer collaboration over contract negotiation (Indian B2B still relationship-driven), and responding to change over following a plan (Indian market changes rapidly). Scrum vs Kanban for Indian startups: Scrum works for teams with stable capacity and defined goals, Kanban suits teams with variable work and support responsibilities, Scrumban hybrid works for most early-stage startups. Sprint duration recommendations: 1-week sprints for early-stage rapid iteration, 2-week sprints for established product development, and avoid 3-4 week sprints in fast-moving markets. Role definitions: Product Owner (usually founder or PM) owns the what and why, Scrum Master (can be rotating) owns the process, and Development Team owns the how. Start simple: implement basic Scrum ceremonies before adding complexity.',
    '["Assess team readiness for agile implementation", "Choose between Scrum, Kanban, or Scrumban based on team context", "Define roles: Product Owner, Scrum Master (or equivalent), Team", "Set sprint duration aligned with team capacity and market pace"]'::jsonb,
    '{"templates": ["Agile Readiness Assessment", "Methodology Selection Guide India", "Role Definition Template", "Sprint Calendar India (with holidays)"], "tools": ["Team Dynamics Analyzer", "Process Selection Decision Tree", "Role RACI Matrix", "Holiday-Adjusted Sprint Planner"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_6_id, 27, 'Sprint Planning for Indian Teams',
    'Effective sprint planning sets teams up for success. Sprint planning meeting structure for India: start on time (enforce discipline), review sprint goal (what problem are we solving for users?), review prioritized backlog (PO presents top items), team estimation and discussion, commitment (what can we realistically deliver?), and sprint goal agreement. User story format: "As a [user type], I want to [action], so that [benefit]" - adapt with Indian examples and contexts. Acceptance criteria: clear definition of done, testable conditions, include edge cases, and consider Indian user contexts (language, devices, network). Story pointing/estimation: use planning poker for team estimation, t-shirt sizing for quick estimates, account for Indian holidays in capacity, and build in buffer for unexpected issues (10-20%). Sprint capacity calculation: available hours per person minus meetings, support, and admin, multiply by velocity factor (0.6-0.8 for realistic planning), and adjust for holidays and leave. Sprint backlog: only committed items go in sprint, protect team from mid-sprint additions, and have clear escalation for urgent items. Remote planning considerations: use video for engagement, share screen with backlog visible, use Miro or similar for estimation, and record for absent team members. Sprint goal: one clear outcome that unifies all stories.',
    '["Create user story template with Indian user context examples", "Conduct sprint planning with proper estimation and capacity calculation", "Define clear sprint goal unifying team efforts", "Set up sprint backlog with committed items only"]'::jsonb,
    '{"templates": ["User Story Template India", "Sprint Planning Agenda", "Capacity Calculator (India Holidays)", "Sprint Goal Framework"], "tools": ["Planning Poker Online Setup", "Story Point Reference Guide", "Sprint Backlog Template", "Remote Planning Facilitation Guide"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_6_id, 28, 'Daily Standups and Sprint Execution',
    'Effective daily rituals keep sprints on track. Daily standup format for Indian teams: 15 minutes maximum, same time daily (morning works best 10-10:15 AM), video on for remote teams (builds connection), three questions: what did you complete yesterday, what will you complete today, any blockers? Standup facilitation: rotate facilitator to build ownership, start on time regardless of attendance, park discussions for after standup, and follow up on blockers immediately. Blockers management: create shared blocker log, assign owner and deadline for resolution, escalate unresolved blockers daily, and protect team time from external interruptions. Sprint execution best practices: work on stories to completion (avoid all stories in progress), update ticket status in real-time, communicate early if deliverables at risk, and collaborate rather than work in silos. Burndown tracking: update daily, review in standup if off-track, adjust scope if needed (remove lowest priority stories), and learn from patterns over sprints. Communication tools for Indian teams: Slack for async communication, WhatsApp for urgent items (but protect from overuse), Jira or Linear for work tracking, and documentation in Notion or Confluence. Sprint execution rhythm: daily standup, midweek check-in for risk assessment, demo preparation throughout (not last day only).',
    '["Establish daily standup rhythm with consistent time and format", "Set up blocker tracking and resolution process", "Implement burndown tracking with daily updates", "Create communication norms for sprint execution (Slack/WhatsApp/Jira)"]'::jsonb,
    '{"templates": ["Standup Facilitation Guide", "Blocker Tracking Template", "Burndown Chart Template", "Communication Norms Document"], "tools": ["Standup Timer Setup", "Jira/Linear Configuration Guide", "Burndown Analysis Tool", "Slack Channel Structure"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_6_id, 29, 'Sprint Reviews and Retrospectives',
    'Sprint reviews and retrospectives drive continuous improvement. Sprint review (demo) format: invite stakeholders (30-60 minutes), demo completed work against sprint goal, celebrate achievements, gather feedback, and discuss upcoming priorities. Demo best practices for India: prepare demos in advance (no surprises), present from user perspective not technical view, include before/after comparisons, and record for absent stakeholders. Stakeholder management in reviews: set expectations on what is demo vs work-in-progress, manage feature requests (add to backlog, not sprint), and build trust through consistent delivery. Retrospective formats that work in India: Start-Stop-Continue (simple and effective), 4Ls (Liked, Learned, Lacked, Longed for), Sailboat (anchors, wind, rocks, island), and Team Radar (score on key dimensions). Retrospective facilitation for Indian teams: create psychological safety (no blame), give time for written reflection before discussion (helps introverted team members), ensure all voices heard (not just senior), and focus on actionable improvements. Action items from retrospectives: limit to 2-3 specific actions, assign owner and deadline, track in next retrospective, and celebrate improvements made. Retrospective frequency: every sprint for active teams, can extend to monthly for mature teams, and special retrospective after major releases or incidents.',
    '["Conduct sprint review demo with stakeholder participation", "Facilitate retrospective using format suited for team", "Create retrospective action items with clear ownership", "Track retrospective action implementation across sprints"]'::jsonb,
    '{"templates": ["Sprint Review Agenda", "Demo Preparation Checklist", "Retrospective Format Library", "Retro Action Tracker"], "tools": ["Demo Recording Setup", "Miro Retrospective Templates", "Action Item Follow-up System", "Team Health Dashboard"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_6_id, 30, 'Continuous Improvement and Velocity',
    'Measuring and improving team velocity drives sustainable productivity. Velocity tracking: measure story points completed per sprint, track trend over 4-6 sprints, use for sprint planning (not performance evaluation), and expect variation (15-20% normal). Velocity improvement levers: reduce meeting overhead, minimize context switching, invest in automation, address technical debt, and improve estimation accuracy. Common velocity patterns in Indian startups: initial sprints underdelivered (learning phase), velocity improves 20-30% in first 3 months, plateau after 6 months (optimize or restructure), and festival seasons impact velocity (plan accordingly). Team health indicators beyond velocity: cycle time (story start to done), lead time (story created to done), defect rate (bugs introduced), and team satisfaction (regular pulse checks). Continuous improvement practices: kaizen mindset (small improvements daily), invest 10-20% of capacity in improvement work, rotate tech debt and improvement ownership, and celebrate improvement wins. Scaling agile for growing teams: add teams carefully (communication overhead), maintain team autonomy, align on shared goals, and coordinate dependencies explicitly. Agile transformation pitfalls: don''t cargo cult practices without understanding why, adapt to Indian context, focus on outcomes not rituals, and build team ownership of process.',
    '["Implement velocity tracking across 4-6 sprints for trend analysis", "Identify top 3 velocity improvement opportunities from retrospectives", "Set up team health metrics: cycle time, lead time, defect rate, satisfaction", "Create continuous improvement backlog with regular investment"]'::jsonb,
    '{"templates": ["Velocity Tracking Dashboard", "Team Health Metrics Template", "Improvement Backlog Format", "Agile Maturity Assessment"], "tools": ["Velocity Trend Analyzer", "Cycle Time Calculator", "Team Satisfaction Survey", "Improvement Impact Tracker"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 7: Product-Market Fit (Days 31-35)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_7_id, v_product_id, 'Product-Market Fit', 'Measure and achieve product-market fit in Indian market conditions', 7, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_7_id, 31, 'Understanding Product-Market Fit',
    'Product-market fit (PMF) in India requires different indicators than Western markets. PMF definition: when your product satisfies strong market demand. Marc Andreessen: "you can always feel when PMF is not happening." Indian market PMF nuances: price sensitivity means customers may use but not pay, adoption may be metro-only initially (validate Tier 2 PMF separately), and family/social influence may drive usage without individual PMF. PMF indicators for Indian market: organic growth (word-of-mouth referrals), retention (users returning without marketing spend), willingness to pay (not just usage), emotional attachment (would they be upset if product disappeared?), and expansion (users finding new use cases). Quantitative PMF benchmarks India: Sean Ellis test - 40%+ would be "very disappointed" if product no longer available, retention - D30 >20% for consumer, >40% for SaaS, NPS >40 for India (cultural lower baseline), and organic acquisition >40% of new users. Warning signs of false PMF: usage driven by heavy marketing spend, customers using but churning quickly, usage limited to free tier only, single customer segment (usually early adopters) only, and metrics flat when marketing paused. PMF is not binary: levels from "some users love it" to "market pulling product out of you." Track your PMF level and trajectory.',
    '["Define PMF indicators specific to your product and Indian market segment", "Set up tracking for retention, organic growth, and Sean Ellis metric", "Establish baseline measurements across all PMF indicators", "Create PMF dashboard tracking trajectory over time"]'::jsonb,
    '{"templates": ["PMF Indicator Framework India", "Sean Ellis Survey Template", "PMF Dashboard Template", "False PMF Warning Checklist"], "tools": ["Retention Calculator", "Organic vs Paid Acquisition Tracker", "NPS India Benchmarking", "PMF Trajectory Analyzer"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_7_id, 32, 'Measuring Product-Market Fit',
    'Systematic PMF measurement provides clarity on where you stand. Sean Ellis test implementation: survey customers who have used product 2+ times, ask "How would you feel if you could no longer use [product]?" with options: Very disappointed, Somewhat disappointed, Not disappointed. Threshold: 40%+ "very disappointed" indicates PMF. NPS (Net Promoter Score) for India: ask "How likely are you to recommend [product] to a friend or colleague?" 0-10 scale. Detractors (0-6), Passives (7-8), Promoters (9-10). NPS = % Promoters - % Detractors. India benchmark: NPS >20 is good, >40 is excellent (lower than Western due to cultural rating patterns). Retention cohort analysis: track users by cohort (signup week/month), measure return usage at D1, D7, D30, D90, identify if retention curve flattens (good) or continues declining (PMF risk). PMF by segment: measure PMF metrics by user segment, some segments may have PMF while others don''t, double down on PMF segments, iterate on others. Qualitative PMF signals: unsolicited testimonials and referrals, users finding workarounds for missing features, users describing product to others accurately, and customer support requests about advanced features (not basics). Leading vs lagging indicators: retention is leading (early signal), revenue is lagging (confirms PMF), and focus on leading indicators early.',
    '["Implement Sean Ellis survey with proper timing and audience selection", "Set up NPS tracking with quarterly measurement cadence", "Build retention cohort analysis for D1, D7, D30, D90 metrics", "Segment PMF measurements by customer type, geography, and acquisition channel"]'::jsonb,
    '{"templates": ["Sean Ellis Survey Implementation Guide", "NPS Survey Template India", "Cohort Analysis Template", "Segment-wise PMF Tracker"], "tools": ["Survey Distribution Automation", "NPS Calculator India", "Retention Cohort Builder", "PMF Segment Comparison Matrix"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_7_id, 33, 'Achieving Product-Market Fit Faster',
    'Accelerate the path to PMF with focused strategies. PMF acceleration strategies: narrow your target market (find smallest viable segment with acute pain), increase customer contact (talk to users daily during pre-PMF), shorten feedback loops (release weekly, learn weekly), focus on core value (remove distractions, perfect one thing), and iterate on positioning (same product, different messaging may find PMF). The bowling pin strategy: identify one narrow segment to dominate (the head pin), achieve PMF there first, then expand to adjacent segments. Works well in India given market fragmentation. Customer development intensity: pre-PMF, spend 30-50% of time with customers, every team member should talk to customers regularly, embed learning into product decisions. Pivot vs persevere: pivot if core hypothesis is wrong, persevere if execution needs improvement, use data to decide (not opinion). Pivot types: customer segment pivot (same product, different customer), problem pivot (same customer, different problem), solution pivot (same problem, different solution), and channel pivot (different distribution). Signs you are approaching PMF: acquisition becomes easier, retention improves without changes, customers become advocates, you can predict user behavior, and unit economics start working.',
    '["Apply bowling pin strategy: identify your head-pin segment to dominate first", "Increase customer contact to 30% of time during pre-PMF phase", "Create weekly release and learning cycle for faster iteration", "Define pivot triggers: what would need to be true to pivot vs persevere"]'::jsonb,
    '{"templates": ["Bowling Pin Strategy Canvas", "Customer Contact Calendar", "Weekly Learning Cycle Template", "Pivot Decision Framework"], "tools": ["Segment Prioritization Matrix", "Customer Time Tracker", "Release Velocity Monitor", "Pivot vs Persevere Analyzer"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_7_id, 34, 'PMF for Indian Market Segments',
    'Different Indian market segments have different PMF thresholds and signals. Metro PMF vs Tier 2 PMF: metro users may show PMF but Tier 2 may not (different needs, constraints), test both segments separately, and be clear which segment you are optimizing for. PMF by income segment: premium segment (higher LTV, easier PMF but smaller market), mass market (harder PMF, need strong unit economics, massive scale), and decide your target and optimize accordingly. PMF for vernacular users: may need separate validation, interface language is just first step (cultural context matters), and test with vernacular-first users, not English speakers using vernacular. B2B PMF in India: longer sales cycles mean delayed PMF signals, relationship-driven sales can mask product issues, measure customer expansion and referrals as signals, and enterprise vs SMB have different PMF indicators. PMF timing for Indian market: consumer products typically 6-12 months to PMF, B2B products 12-24 months, marketplaces 18-36 months (chicken-egg), and fintech varies based on regulatory requirements. Post-PMF transitions: PMF in one segment is just the beginning, plan expansion to adjacent segments, PMF may not transfer automatically (re-validate), and build PMF playbook for repeatable expansion.',
    '["Validate PMF separately for metro vs Tier 2 segments", "Test PMF with vernacular-first users if targeting mass market", "Establish appropriate PMF timeline expectations for your business model", "Create PMF playbook documenting what works for segment expansion"]'::jsonb,
    '{"templates": ["Segment-wise PMF Validation Plan", "Vernacular User PMF Testing Guide", "PMF Timeline Framework by Model", "PMF Expansion Playbook"], "tools": ["Metro vs Tier Comparison Dashboard", "Vernacular User Recruitment", "PMF Stage Identifier", "Expansion Readiness Assessment"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_7_id, 35, 'Scaling After Product-Market Fit',
    'PMF is the starting point for scaling, not the end goal. Pre-scale checklist: PMF confirmed across target segments, unit economics validated (LTV > 3x CAC), retention curves flattened, organic growth demonstrated, and team capacity for growth. Scaling prematurely is the #1 startup killer: without PMF, scaling amplifies problems, burns cash faster, and builds wrong things at scale. Post-PMF priorities shift: from finding PMF to expanding PMF, from product iteration to growth systems, from founder-led sales to scalable acquisition, and from team building to team scaling. Growth levers post-PMF: product-led growth (virality, referrals), marketing-led growth (paid acquisition, content), sales-led growth (outbound, partnerships), and expansion-led growth (upsell, cross-sell). Indian market scaling considerations: geographic expansion (metro to Tier 2 to Tier 3), language expansion (English to Hindi to regional), segment expansion (early adopters to mainstream), and channel expansion (digital to offline/hybrid). Scaling team for India: hire ahead of growth (3-6 months), document processes before scaling, build training and onboarding systems, and culture preservation during growth. Funding implications: PMF is primary milestone for Series A in India, demonstrated PMF significantly improves terms and options, and use PMF evidence in fundraising narrative.',
    '["Complete pre-scale checklist validating PMF, unit economics, and retention", "Identify primary growth lever for your business model", "Create geographic and segment expansion roadmap for India", "Build fundraising narrative around PMF evidence and scaling plan"]'::jsonb,
    '{"templates": ["Pre-Scale Readiness Checklist", "Growth Lever Selection Framework", "India Expansion Roadmap Template", "PMF Evidence for Fundraising"], "tools": ["Unit Economics Calculator", "Scaling Readiness Assessment", "Expansion Prioritization Matrix", "Funding Milestone Tracker"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 8: Feature Prioritization (Days 36-40)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_8_id, v_product_id, 'Feature Prioritization', 'Master frameworks for prioritizing features in resource-constrained Indian startup environments', 8, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_8_id, 36, 'Prioritization Frameworks for Indian Startups',
    'Resource constraints in Indian startups make prioritization critical. Framework options: RICE (Reach, Impact, Confidence, Effort), ICE (Impact, Confidence, Ease), Value vs Effort matrix, Kano Model, and Opportunity Scoring. RICE framework adapted for India: Reach - how many Indian users affected per quarter (consider segment-specific reach), Impact - effect on key metric (1-3 scale with India conversion benchmarks), Confidence - evidence level (higher bar given aspirational feedback tendency), Effort - person-months with Indian dev cost consideration. ICE scoring: Impact (1-10) x Confidence (1-10) x Ease (1-10) = ICE score. Simpler than RICE, good for quick prioritization. Value vs Effort matrix: plot features on 2x2 (high/low value, high/low effort), prioritize: Quick Wins (high value, low effort) first, Big Bets (high value, high effort) with planning, Money Pit (low value, high effort) avoid, and Fill-Ins (low value, low effort) as time permits. Kano model for India: Must-haves (expected, no delight - trust features, payment options), Performance features (more is better - speed, selection), and Delighters (unexpected value - vernacular support, personalization). Indian startup prioritization principles: survival features first (revenue, retention), trust features are must-haves, and fancy features come later.',
    '["Score top 20 features using RICE framework with India adaptations", "Create value vs effort matrix for visual prioritization", "Identify must-haves, performers, and delighters using Kano model", "Establish prioritization criteria weights for your specific context"]'::jsonb,
    '{"templates": ["RICE Scoring Template India", "ICE Framework Calculator", "Value vs Effort Matrix", "Kano Classification Guide"], "tools": ["Feature Scoring Dashboard", "Priority Score Calculator", "Quick Win Identifier", "Framework Selection Guide"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_8_id, 37, 'Pricing Strategy for Indian Market',
    'Pricing in India requires understanding the unique value perception and willingness to pay. Indian pricing psychology: extreme price sensitivity (10% difference changes behavior), value-consciousness (quality per rupee), round number preference (₹99, ₹199, ₹499), and EMI/subscription preference for larger amounts. Pricing research methods: Van Westendorp Price Sensitivity Meter, Conjoint analysis, A/B testing prices, and direct surveys (less reliable in India due to stated vs actual gap). Van Westendorp for India: ask four questions - at what price would it be so cheap you''d doubt quality? When does it start to seem like a bargain? When does it start to seem expensive? When is it too expensive? Plot responses to find acceptable price range. Pricing models for India: Freemium (free tier with paid upgrades - high conversion needed), Subscription (growing acceptance, monthly preferred over annual), Pay-per-use (aligns with Indian value consciousness), and Tiered pricing (good-better-best with clear differentiation). Indian SaaS pricing: typically 50-70% of US pricing, annual discount of 20-30% effective, and INR pricing critical (avoid USD). Consumer app pricing: ₹99-499/month for subscriptions, one-time purchases up to ₹2000 for committed users, and in-app purchases for gaming/entertainment. Test pricing early: pricing affects PMF assessment, wrong pricing masks product issues.',
    '["Conduct Van Westendorp pricing research with 100+ target customers", "Design pricing model aligned with Indian payment preferences", "Create pricing tiers with clear value differentiation", "A/B test pricing with different segments and price points"]'::jsonb,
    '{"templates": ["Van Westendorp Survey Template", "Pricing Model Comparison India", "Pricing Tier Framework", "A/B Pricing Test Design"], "tools": ["Price Sensitivity Analyzer", "Pricing Calculator India", "Competitive Pricing Tracker", "Price Elasticity Calculator"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_8_id, 38, 'Stakeholder Management in Prioritization',
    'Managing competing priorities from stakeholders is a critical PM skill. Common stakeholder types: founders (vision and strategy focus), engineering (technical concerns and capacity), sales (customer requests and deals), marketing (launch and growth features), customer success (retention and support features), and investors (growth metrics and milestones). Stakeholder alignment techniques: regular product reviews with prioritization rationale, data-driven justification for decisions, explicit trade-off discussions, and say no with alternatives. Feature request management: capture all requests systematically, evaluate against prioritization framework, communicate decisions with rationale, and close the loop when features ship. Sales vs product tension: sales brings customer requests, evaluate against broader strategy, single customer features should be paid development, and build for segments, not individual customers. Founder-PM alignment: founders often have strong opinions, respect vision while providing data perspective, pick battles carefully, and align on decision rights. Engineering partnership: involve engineers in prioritization discussions, respect technical constraints and debt, balance feature delivery with platform health, and build trust through realistic commitments. Saying no effectively: acknowledge the request and requestor, explain prioritization rationale, offer alternatives or timeline, and follow up when relevant.',
    '["Map all stakeholders with their interests and influence on prioritization", "Establish feature request intake and evaluation process", "Create stakeholder communication cadence for priority decisions", "Develop framework for saying no while maintaining relationships"]'::jsonb,
    '{"templates": ["Stakeholder Mapping Matrix", "Feature Request Form", "Priority Communication Template", "Trade-off Discussion Framework"], "tools": ["Stakeholder Influence Analyzer", "Request Tracking System", "Decision Log Template", "Alignment Meeting Agenda"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_8_id, 39, 'Roadmap Creation and Communication',
    'Effective roadmaps align teams and stakeholders on product direction. Roadmap types: timeline-based (quarters with specific features), now-next-later (flexible timeframes), theme-based (strategic initiatives), and outcome-based (goals to achieve). Roadmap timeframes for Indian startups: now (current sprint, committed), next (next 1-2 sprints, high confidence), later (2-6 months, directional), and vision (6-12 months, aspirational). Roadmap content: include themes/initiatives (not just features), tie to business objectives, show dependencies, and indicate confidence levels. Roadmap for different audiences: executive roadmap (strategic themes, outcomes), team roadmap (detailed features, timelines), and customer roadmap (upcoming capabilities, value delivery). Roadmap communication: regular reviews (monthly for team, quarterly for stakeholders), update proactively when changes occur, explain changes with rationale, and celebrate delivered items. Roadmap mistakes to avoid: over-committing to specific dates, treating roadmap as contract, not updating when plans change, and feature-level detail too far out. Living document: roadmap should evolve based on learning, review and update at least monthly, version history for accountability, and clear ownership of updates.',
    '["Create now-next-later roadmap for next 6 months", "Develop audience-specific roadmap views (exec, team, customer)", "Establish roadmap review and update cadence", "Build roadmap communication and change management process"]'::jsonb,
    '{"templates": ["Now-Next-Later Roadmap Template", "Executive Roadmap View", "Team Roadmap Template", "Customer-Facing Roadmap"], "tools": ["Roadmap Tool Comparison", "Dependency Mapping Guide", "Roadmap Change Log", "Communication Cadence Planner"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_8_id, 40, 'Prioritization in Practice',
    'Apply prioritization frameworks to real product decisions. Weekly prioritization ritual: review new requests and data, reassess current priorities, adjust sprint planning input, and communicate changes. Data-driven prioritization: use analytics to identify drop-off points, A/B test to validate assumptions, measure feature usage post-launch, and deprioritize low-usage features. Feature factory trap: shipping features without measuring impact, quantity over quality, no time for iteration, and no learning from what ships. Avoid by: setting success criteria before building, measuring impact after launch, allocating time for iteration, and killing features that don''t perform. Prioritization for different stages: pre-PMF (optimize for learning, focus on core value), growth stage (optimize for scalable features), and scale stage (optimize for efficiency and expansion). Emergency prioritization: when urgent issues arise, have clear criteria for interrupting planned work, time-box the emergency, and return to planned priorities. Quarterly planning: review strategic priorities, major initiatives for quarter, capacity allocation, and team alignment. Prioritization documentation: maintain decision log with rationale, reference for future decisions, and accountability for outcomes.',
    '["Implement weekly prioritization review ritual", "Set up feature success measurement for all shipped features", "Create emergency prioritization criteria and escalation process", "Plan next quarter using strategic prioritization framework"]'::jsonb,
    '{"templates": ["Weekly Prioritization Review Agenda", "Feature Success Metrics Template", "Emergency Prioritization Criteria", "Quarterly Planning Template"], "tools": ["Prioritization Meeting Facilitation Guide", "Feature Impact Dashboard", "Decision Log System", "Quarterly Review Framework"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 9: Growth Experiments (Days 41-45)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_9_id, v_product_id, 'Growth Experiments', 'Design and run growth experiments optimized for Indian market conditions and user behaviors', 9, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_9_id, 41, 'Growth Experimentation Fundamentals',
    'Systematic experimentation is the foundation of sustainable growth. Experimentation mindset: every growth initiative is a hypothesis to test, failure is learning (not failure), measure everything, and compound small wins. Growth experiment structure: hypothesis (if we do X, we expect Y because Z), metric to move (specific, measurable), success criteria (what result validates hypothesis), and timeline (experiment duration). Experiment types: A/B tests (two versions, statistical comparison), multivariate tests (multiple variables), before/after tests (measure change impact), and cohort experiments (compare user groups). Indian market experimentation considerations: larger sample sizes needed for significance (India variance is high), segment experiments by tier city, test vernacular separately, and account for seasonality (festivals, exams, monsoon). Statistical significance for India: minimum 1000 users per variant for consumer, 100 for B2B, 95% confidence level, and 7-14 day minimum duration. Experimentation velocity: aim for 2-4 experiments per week in growth phase, document all experiments (including failures), and build experiment backlog from data insights. Common experiment mistakes: stopping too early, testing too many things at once, ignoring segment differences, and not documenting learnings.',
    '["Create experiment documentation template with hypothesis, metrics, and success criteria", "Build experiment backlog with 20+ ideas prioritized by impact and effort", "Set up experimentation infrastructure (feature flags, analytics)", "Establish experiment review cadence (weekly experiment reviews)"]'::jsonb,
    '{"templates": ["Growth Experiment Template", "Experiment Backlog Prioritization", "Statistical Significance Guide India", "Experiment Documentation Format"], "tools": ["Feature Flag Implementation Guide", "Sample Size Calculator India", "Experiment Tracking Dashboard", "Learning Repository Setup"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_9_id, 42, 'Acquisition Experiments for India',
    'Customer acquisition experiments optimized for Indian channels and behaviors. Indian acquisition channels: organic search (Google dominates), paid search (Google Ads), social media (Instagram, Facebook, YouTube), WhatsApp (viral and referral), influencer marketing, and offline (events, retail). Acquisition experiment ideas: landing page optimization (headline, CTA, trust signals), ad creative testing (vernacular vs English, video vs static), channel expansion (test new channels with limited budget), and audience targeting (segment discovery and optimization). WhatsApp-specific experiments: viral loops (share to unlock), referral programs (two-sided incentives), and WhatsApp Ads (click-to-WhatsApp). Indian-specific acquisition tactics: regional language ads (30-50% lower CPC), festival-themed campaigns, and cricket-related content during IPL/World Cup. Landing page experiments for India: trust signals (testimonials, certifications, media logos), price anchoring (show savings, EMI options), urgency (limited time, stock running out), and social proof (user count, regional popularity). Referral program experiments: incentive amount testing, one-sided vs two-sided rewards, referral timing (when to prompt), and mechanic testing (link, code, WhatsApp share). CAC benchmarks for experiments: know your current CAC by channel, set targets for improvement, measure incrementality (not just cost per conversion).',
    '["Design 5 landing page experiments targeting trust and conversion", "Create WhatsApp referral experiment with A/B tested incentives", "Test regional language ads against English for 2-3 key segments", "Set up CAC tracking by channel to measure experiment impact"]'::jsonb,
    '{"templates": ["Landing Page Experiment Template", "Referral Program Experiment Design", "Ad Creative Testing Framework", "Channel CAC Tracker"], "tools": ["Landing Page Optimization Checklist", "WhatsApp Viral Loop Builder", "Regional Ad Copy Guide", "Incrementality Calculator"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_9_id, 43, 'Activation Experiments',
    'Improving activation converts more acquired users into active users. Activation definition: the moment a user experiences your core value for the first time. Define your aha moment clearly. Activation metrics: time to first value action, completion rate of onboarding, D1 retention (strong activation signal), and activation rate by cohort. Indian activation challenges: users may sign up but not engage, feature discovery is low (users miss value), onboarding abandonment is high (complex flows), and language/context gaps cause confusion. Onboarding experiments: steps reduction (every step loses 20% users), progress indicators (completion motivation), personalization (relevant content from start), and quick wins (immediate value before setup). Indian onboarding adaptations: mobile number + OTP (preferred over email), WhatsApp login option, vernacular onboarding for non-English users, and video tutorials (visual learning preference). Activation experiment ideas: empty state optimization (what users see first), guided tours (introduce features contextually), early win mechanics (help users succeed quickly), and personalized recommendations (reduce decision fatigue). Measure activation experiments: activation rate change, time to activate, and downstream retention impact.',
    '["Define your product aha moment and activation metric clearly", "Design 5 onboarding experiments to reduce friction and increase completion", "Test mobile number vs email signup for activation rate impact", "Create guided tour experiment introducing top 3 features contextually"]'::jsonb,
    '{"templates": ["Activation Definition Framework", "Onboarding Experiment Checklist", "Empty State Design Guide", "Guided Tour Scripting Template"], "tools": ["Onboarding Funnel Analyzer", "Time to Activation Tracker", "Feature Discovery Heatmap", "Activation Cohort Dashboard"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_9_id, 44, 'Retention Experiments',
    'Retention experiments have the highest ROI for sustainable growth. Retention experiment categories: habit formation (build daily/weekly use patterns), re-engagement (bring back churning users), value deepening (increase usage intensity), and loyalty (reward and retain best users). Indian retention patterns: price-driven churn is common (value perception is key), seasonal usage patterns (festivals, events affect behavior), and competition-driven churn (new alternatives). Habit formation experiments: trigger optimization (when to prompt usage), variable rewards (gamification, surprise benefits), investment mechanics (users invest in product), and social connection (community features). Re-engagement experiments: notification optimization (timing, content, frequency), email campaigns (nurture sequences), WhatsApp re-engagement (high open rates), and win-back offers (discounts for churned users). Retention notification experiments for India: test vernacular notifications, optimal timing (evening 7-9 PM often best), frequency (avoid fatigue, 3-5/week max), and personalization (name, activity-based content). Cohort-based retention experiments: identify at-risk cohorts (behavior patterns), targeted interventions for each cohort, and measure cohort-specific retention improvement. Retention benchmarks to target: D1 >40%, D7 >20%, D30 >10% for consumer, higher for B2B.',
    '["Map user journey identifying key retention moments and drop-off points", "Design habit formation experiment with triggers, rewards, and investments", "Create re-engagement experiment sequence for churned users", "Set up retention cohort analysis to identify at-risk users"]'::jsonb,
    '{"templates": ["Retention Experiment Framework", "Habit Loop Design Template", "Re-engagement Campaign Template", "Cohort Risk Identification Guide"], "tools": ["Churn Prediction Model", "Notification A/B Testing Setup", "Re-engagement Sequence Builder", "Retention Cohort Dashboard"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_9_id, 45, 'Monetization Experiments',
    'Monetization experiments optimize revenue without sacrificing user experience. Monetization experiment types: pricing experiments (price points, tiers), packaging experiments (what features in each tier), timing experiments (when to prompt payment), and model experiments (subscription vs one-time vs freemium). Indian monetization considerations: payment method preferences (UPI dominant, COD for e-commerce), EMI options (increase conversion for >₹3000), trial sensitivity (free trials expected), and discount expectations (festive offers expected). Paywall experiments: paywall placement (when to show payment prompt), feature gating (which features to put behind paywall), trial length (7 vs 14 vs 30 days), and soft vs hard paywalls (limited access vs complete block). Pricing presentation experiments: anchor pricing (show original price crossed out), savings framing (you save ₹X), time-based framing (only ₹X per day), and social framing (join X other users). Indian payment optimization: UPI integration is table stakes, multiple payment options increase conversion, saved payment methods reduce friction, and trust badges near payment button. Upsell experiments: timing of upsell (when users hit limits), value proposition (what justifies upgrade), and bundling (product combinations).',
    '["Design pricing experiment testing 2-3 price points with different segments", "Create paywall timing experiment finding optimal conversion moment", "Test UPI-first payment flow against multi-option for conversion impact", "Implement upsell experiment triggered by usage patterns"]'::jsonb,
    '{"templates": ["Pricing Experiment Design Template", "Paywall Strategy Framework", "Payment Flow Optimization Checklist", "Upsell Trigger Framework"], "tools": ["Price Elasticity Tester", "Paywall Conversion Dashboard", "Payment Method Analytics", "Revenue Impact Calculator"]}'::jsonb, 75, 50, 5, NOW(), NOW());

    -- Module 10: Product Analytics (Days 46-50)
    INSERT INTO "Module" (id, "productId", title, description, "orderIndex", "createdAt", "updatedAt")
    VALUES (v_mod_10_id, v_product_id, 'Product Analytics', 'Build and leverage product analytics for data-driven decisions in Indian startup context', 10, NOW(), NOW());

    INSERT INTO "Lesson" (id, "moduleId", day, title, "briefContent", "actionItems", resources, "estimatedTime", "xpReward", "orderIndex", "createdAt", "updatedAt") VALUES
    (gen_random_uuid()::text, v_mod_10_id, 46, 'Product Analytics Fundamentals',
    'Product analytics transforms data into actionable product insights. Analytics maturity levels: Level 1 (basic tracking - pageviews, sessions), Level 2 (event tracking - user actions), Level 3 (user analytics - cohorts, funnels), Level 4 (predictive analytics - ML-driven insights). Key analytics metrics: acquisition (new users, sources, cost), activation (onboarding completion, time to value), engagement (DAU, MAU, stickiness), retention (cohort retention, churn rate), and revenue (ARPU, LTV, conversion rate). Analytics tools for Indian startups: Mixpanel (powerful, good free tier), Amplitude (enterprise-grade, free tier available), PostHog (open-source, self-hosted option), Google Analytics 4 (free, limited for product analytics), and CleverTap (strong in India, mobile-focused). Tool selection criteria: budget, technical requirements, integration needs, team capability, and data ownership preferences. Indian analytics considerations: segment by tier city (behavior varies significantly), track language preference impact, monitor device and network quality effects, and account for shared device usage. Analytics implementation: event taxonomy (naming conventions), tracking plan (what to track), implementation (code-level), QA (verify data quality), and documentation (maintain tracking docs).',
    '["Assess current analytics maturity level and set improvement targets", "Select and implement analytics tool based on needs and budget", "Create event taxonomy with consistent naming conventions", "Build tracking plan covering all key user actions and funnels"]'::jsonb,
    '{"templates": ["Analytics Maturity Assessment", "Tool Selection Framework India", "Event Taxonomy Template", "Tracking Plan Document"], "tools": ["Mixpanel Implementation Guide", "PostHog Setup Tutorial", "Event Naming Convention Generator", "Tracking QA Checklist"]}'::jsonb, 75, 50, 1, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_10_id, 47, 'Funnel Analysis and Optimization',
    'Funnel analysis reveals where users drop off and optimization opportunities. Core product funnels: acquisition funnel (visitor to signup), onboarding funnel (signup to activated), conversion funnel (activated to paid), and feature funnels (specific feature adoption). Funnel analysis process: define funnel steps clearly, measure conversion at each step, identify biggest drop-offs, diagnose why users drop off, and test improvements. Indian funnel benchmarks: landing page to signup 2-5%, signup to onboarding complete 40-60%, onboarding to activated 30-50%, and free to paid 2-5% (consumer), 10-20% (B2B). Diagnosing drop-offs: session recordings (Hotjar, FullStory), user surveys at drop-off points, usability testing, and analytics deep-dive by segment. Indian-specific funnel issues: payment step drop-off (UPI/payment failures), KYC step abandonment (document upload friction), language/context confusion, and trust concerns at key moments. Funnel optimization tactics: reduce steps, improve copy/UX, add progress indicators, optimize for mobile, and test aggressively. Segment funnel analysis: view funnels by user segment (metro vs tier 2, new vs returning), identify segment-specific issues, and personalize flows for different segments.',
    '["Map all core funnels with clear step definitions and success metrics", "Calculate conversion rates at each step with Indian benchmarks comparison", "Identify top 3 funnel drop-offs using session recordings and surveys", "Design funnel experiments to improve conversion at key drop-off points"]'::jsonb,
    '{"templates": ["Funnel Mapping Template", "Drop-off Diagnosis Checklist", "Funnel Experiment Prioritization", "Segment Funnel Comparison"], "tools": ["Funnel Visualization Dashboard", "Drop-off Survey Template", "Session Recording Analysis Guide", "Conversion Rate Calculator"]}'::jsonb, 75, 50, 2, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_10_id, 48, 'Cohort Analysis and Retention Tracking',
    'Cohort analysis reveals how user behavior evolves over time. Cohort types: acquisition cohorts (grouped by signup date), behavioral cohorts (grouped by actions taken), and segment cohorts (grouped by user attributes). Retention cohort construction: define user cohort (e.g., all signups in Week 1), define retention action (e.g., any activity), measure retention at intervals (D1, D7, D30, D60, D90). Interpreting retention curves: steep early drop is normal (finding fit), flatten indicates retention (users finding value), continued decline indicates churn problem. Indian retention patterns: higher early churn (experimentation culture), seasonal patterns (festivals, events), and price sensitivity causing churn at renewal. Cohort comparison: compare cohorts over time to see if retention improving, compare segments within cohort to identify patterns, and identify successful cohorts to understand what drives retention. Retention cohort actions: identify at-risk cohorts for intervention, celebrate improving cohorts, and learn from best-performing cohorts. Advanced cohort analysis: cohort revenue analysis (revenue per cohort over time), feature adoption cohorts (users who adopt feature X vs not), and experiment cohorts (users in experiment groups).',
    '["Create weekly acquisition cohort analysis for last 12 weeks", "Build retention curve visualization with D1, D7, D30, D60, D90 points", "Compare retention across user segments (tier city, acquisition source)", "Identify best and worst performing cohorts and analyze differences"]'::jsonb,
    '{"templates": ["Cohort Analysis Template", "Retention Curve Guide", "Segment Cohort Comparison", "Cohort Learning Framework"], "tools": ["Cohort Visualization Builder", "Retention Curve Calculator", "Segment Comparison Dashboard", "Cohort Action Planner"]}'::jsonb, 75, 50, 3, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_10_id, 49, 'Building Product Dashboards',
    'Effective dashboards enable data-driven decisions across the team. Dashboard principles: answer specific questions, actionable insights over vanity metrics, appropriate update frequency, and accessible to stakeholders. Dashboard types: executive dashboard (KPIs, trends, health), team dashboard (operational metrics, experiments), and feature dashboard (specific feature performance). Indian startup core metrics dashboard: Daily Active Users (DAU), Monthly Active Users (MAU), DAU/MAU ratio (stickiness), new user signups, activation rate, D7 and D30 retention, revenue metrics (MRR/GMV), and CAC/LTV. Dashboard design: most important metrics at top, clear visualizations (avoid complexity), comparison to previous period/target, and drill-down capability for investigation. Dashboard tools for India: Metabase (free, self-hosted), Mixpanel/Amplitude dashboards (built-in), Google Data Studio (free, integrates with GA), and Tableau/Looker (enterprise). Dashboard cadence: real-time for operational monitoring, daily for team reviews, weekly for leadership reviews, and monthly for board reporting. Dashboard discipline: review dashboards regularly, take action on concerning trends, update dashboards as product evolves, and ensure data quality.',
    '["Design executive KPI dashboard with top 10 metrics and trends", "Build team operational dashboard with daily metrics and experiments", "Create feature-specific dashboards for top 3 features", "Establish dashboard review cadence and action process"]'::jsonb,
    '{"templates": ["Executive Dashboard Template", "Team Dashboard Design", "Feature Dashboard Framework", "Dashboard Review Agenda"], "tools": ["Metabase Setup Guide", "Dashboard Best Practices Checklist", "Visualization Selection Guide", "Alert Configuration Template"]}'::jsonb, 75, 50, 4, NOW(), NOW()),

    (gen_random_uuid()::text, v_mod_10_id, 50, 'Data-Driven Product Culture',
    'Building a data-driven culture is as important as the analytics infrastructure. Data-driven culture characteristics: decisions backed by data, hypotheses before experiments, learning from both success and failure, and accessible data for all team members. Common blockers to data culture: data quality issues (garbage in, garbage out), analysis paralysis (over-thinking vs acting), HiPPO (highest paid person opinion) overriding data, and lack of analytics skills in team. Building data culture: start with leadership modeling data use, make data accessible and understandable, celebrate data-driven wins, and invest in analytics training. Analytics skills for product teams: basic SQL for data exploration, analytics tool proficiency, statistical literacy (significance, correlation vs causation), and data visualization skills. Indian startup data culture: start simple and build, prioritize trust and adoption over complexity, and democratize data access. Weekly analytics rituals: metrics review meeting (what changed and why), experiment review (what did we learn), funnel review (where are users struggling), and cohort review (how are users evolving). Product analytics career path: establish product analytics role, analytics engineer for data infrastructure, and data scientist for advanced analysis. Continuous improvement: regularly audit data quality, evolve tracking with product, and update dashboards and reports.',
    '["Assess current data-driven culture maturity and identify gaps", "Establish weekly analytics review rituals (metrics, experiments, funnels)", "Create analytics training plan for product team members", "Build data quality monitoring and improvement process"]'::jsonb,
    '{"templates": ["Data Culture Assessment", "Weekly Analytics Review Agenda", "Analytics Training Curriculum", "Data Quality Audit Checklist"], "tools": ["Culture Survey Template", "Meeting Facilitation Guide", "SQL Training Resources", "Data Quality Dashboard"]}'::jsonb, 75, 50, 5, NOW(), NOW());

END $$;

COMMIT;
